{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb290fe-e4cd-4088-b833-e2e1c5788a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a5a6bd-6d5b-498f-8d93-0ce8a4300bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data1 Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35768 entries, 0 to 35767\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   City                  35768 non-null  object \n",
      " 1   Price                 35768 non-null  float64\n",
      " 2   Address               35768 non-null  object \n",
      " 3   Number_Beds           35768 non-null  int64  \n",
      " 4   Number_Baths          35768 non-null  int64  \n",
      " 5   Province              35768 non-null  object \n",
      " 6   Population            35768 non-null  int64  \n",
      " 7   Latitude              35768 non-null  float64\n",
      " 8   Longitude             35768 non-null  float64\n",
      " 9   Median_Family_Income  35768 non-null  float64\n",
      "dtypes: float64(4), int64(3), object(3)\n",
      "memory usage: 2.7+ MB\n",
      "None\n",
      "Data2 Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Price       5000 non-null   int64 \n",
      " 1   Bedrooms    5000 non-null   int64 \n",
      " 2   Bathrooms   5000 non-null   int64 \n",
      " 3   SqFt        5000 non-null   int64 \n",
      " 4   City        5000 non-null   object\n",
      " 5   Province    5000 non-null   object\n",
      " 6   Year_Built  5000 non-null   int64 \n",
      " 7   Type        5000 non-null   object\n",
      " 8   Garage      5000 non-null   int64 \n",
      " 9   Lot_Area    5000 non-null   int64 \n",
      "dtypes: int64(7), object(3)\n",
      "memory usage: 390.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the two datasets\n",
    "data1 = pd.read_csv('/Users/ernestgaisie/Desktop/Housing Prices Project/HouseListings-Top45Cities-10292023-kaggle.csv', encoding='latin1')\n",
    "data2 = pd.read_csv('/Users/ernestgaisie/Desktop/Housing Prices Project/ca_real_estate 2.csv', encoding='latin1')\n",
    "\n",
    "# Display basic info for both datasets\n",
    "print(\"Data1 Info:\")\n",
    "print(data1.info())\n",
    "\n",
    "print(\"Data2 Info:\")\n",
    "print(data2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719dd007-4882-40b3-893f-fcdb65ce7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for province names\n",
    "province_mapping = {\n",
    "    'ON': 'Ontario',\n",
    "    'QC': 'Quebec',\n",
    "    'BC': 'British Columbia',\n",
    "    'AB': 'Alberta',\n",
    "    'MB': 'Manitoba',\n",
    "    'SK': 'Saskatchewan',\n",
    "    'NS': 'Nova Scotia',\n",
    "    'NB': 'New Brunswick',\n",
    "    'NL': 'Newfoundland and Labrador',\n",
    "    'PE': 'Prince Edward Island',\n",
    "    'YT': 'Yukon',\n",
    "    'NT': 'Northwest Territories',\n",
    "    'NU': 'Nunavut'\n",
    "}\n",
    "\n",
    "# Replace province abbreviations with full names\n",
    "data1['Province'] = data1['Province'].replace(province_mapping)\n",
    "data2['Province'] = data2['Province'].replace(province_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7040b862-87df-4c89-8675-246a5cae2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to align datasets\n",
    "rename_map_data1 = {'Number_Beds': 'Bedrooms', 'Number_Baths': 'Bathrooms', 'Price': 'Price_x'}\n",
    "rename_map_data2 = {'Price': 'Price_y'}\n",
    "\n",
    "data1.rename(columns=rename_map_data1, inplace=True)\n",
    "data2.rename(columns=rename_map_data2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a603f8cb-426c-4f66-b8a9-661c884fe802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Type column to 'House' for data1\n",
    "data1['Type'] = 'House'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e48b7-d3b9-4350-bd7d-b1624ae8deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing columns in each dataset\n",
    "missing_columns_data1 = set(data2.columns) - set(data1.columns)\n",
    "missing_columns_data2 = set(data1.columns) - set(data2.columns)\n",
    "\n",
    "# Add missing columns with NaN\n",
    "for col in missing_columns_data1:\n",
    "    data1[col] = np.nan\n",
    "for col in missing_columns_data2:\n",
    "    data2[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3837d39-2b26-49c0-9a48-c403bba35b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40768 entries, 0 to 40767\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   City                  40768 non-null  object \n",
      " 1   Price_x               35768 non-null  float64\n",
      " 2   Address               35768 non-null  object \n",
      " 3   Bedrooms              40768 non-null  int64  \n",
      " 4   Bathrooms             40768 non-null  int64  \n",
      " 5   Province              40768 non-null  object \n",
      " 6   Population            35768 non-null  float64\n",
      " 7   Latitude              35768 non-null  float64\n",
      " 8   Longitude             35768 non-null  float64\n",
      " 9   Median_Family_Income  35768 non-null  float64\n",
      " 10  Type                  40768 non-null  object \n",
      " 11  Garage                5000 non-null   float64\n",
      " 12  Year_Built            5000 non-null   float64\n",
      " 13  Price_y               5000 non-null   float64\n",
      " 14  Lot_Area              5000 non-null   float64\n",
      " 15  SqFt                  5000 non-null   float64\n",
      "dtypes: float64(10), int64(2), object(4)\n",
      "memory usage: 5.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Append datasets\n",
    "combined_data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "# Display combined dataset info\n",
    "print(\"Combined Dataset Info:\")\n",
    "print(combined_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c89c05-fb38-4ac5-aad5-114d40eaab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing Population values: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a mapping of City to Population from data1\n",
    "city_population_map = data1.set_index('City')['Population'].to_dict()\n",
    "\n",
    "# Step 2: Infer Population for missing values in combined_data\n",
    "combined_data['Population'] = combined_data.apply(\n",
    "    lambda row: city_population_map.get(row['City'], row['Population']), axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Handle remaining missing Population values using grouped median\n",
    "if combined_data['Population'].isnull().sum() > 0:\n",
    "    combined_data['Population'] = combined_data.groupby(['Province', 'City'])['Population'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "# Step 4: Final fallback to overall median if Population is still missing\n",
    "if combined_data['Population'].isnull().sum() > 0:\n",
    "    combined_data['Population'].fillna(combined_data['Population'].median(), inplace=True)\n",
    "\n",
    "# Verify the results\n",
    "print(\"Remaining missing Population values:\", combined_data['Population'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f4fa2f-40e8-4d00-b70c-2bcdbd7637ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Price_x and Price_y into a single column\n",
    "combined_data['Price'] = combined_data['Price_x'].combine_first(combined_data['Price_y'])\n",
    "combined_data.drop(['Price_x', 'Price_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ef993e-adfd-4cd3-b24a-3ba60fdc54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to infer\n",
    "columns_to_infer = ['Lot_Area', 'Median_Family_Income']\n",
    "\n",
    "# Grouped inference\n",
    "for col in columns_to_infer:\n",
    "    if col in combined_data.columns:\n",
    "        combined_data[col] = combined_data.groupby(['City', 'Province', 'Bedrooms'])[col].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "\n",
    "# Fallback to broader grouping (Province-level)\n",
    "for col in columns_to_infer:\n",
    "    if col in combined_data.columns and combined_data[col].isnull().sum() > 0:\n",
    "        combined_data[col] = combined_data.groupby(['Province', 'Bedrooms'])[col].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c5c3bd0-bc99-42ec-acc1-dfb3e817f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns left as NaN: Index(['City', 'Address', 'Province', 'Type'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# No changes needed for Garage and Year_Built; leave NaN\n",
    "print(\"Non-numeric columns left as NaN:\", combined_data.select_dtypes(exclude=['float64', 'int64']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07534935-b93d-4bc2-b495-bbcdf72c03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "combined_data.drop(['Latitude', 'Longitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddedb7b-2b4a-4485-8d63-0fe1d73c19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned dataset has been saved as 'combined_data_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "combined_data.to_csv('combined_data_cleaned.csv', index=False)\n",
    "print(\"Final cleaned dataset has been saved as 'combined_data_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be2b8bb6-8203-470d-b3ac-8fe563c3382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c431306-09e9-4905-b8c7-ad830d7c97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined dataset (already preprocessed in previous steps)\n",
    "combined_data = pd.read_csv('combined_data_cleaned.csv')\n",
    "\n",
    "# Display basic information for verification\n",
    "print(\"Dataset Info:\")\n",
    "print(combined_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "785eb88e-7db4-4b89-bdfd-c73ee5155ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: Index(['Bedrooms', 'Bathrooms', 'Population', 'Median_Family_Income', 'Garage',\n",
      "       'Year_Built', 'Lot_Area', 'SqFt'],\n",
      "      dtype='object')\n",
      "Categorical Columns:             City                  Address Province       Type\n",
      "0        Toronto    #318 -20 SOUTHPORT ST  Ontario      House\n",
      "1        Toronto    #818 -60 SOUTHPORT ST  Ontario      House\n",
      "2        Toronto  #714 -859 THE QUEENSWAY  Ontario      House\n",
      "3        Toronto         275 MORTIMER AVE  Ontario      House\n",
      "4        Toronto    #420 -388 RICHMOND ST  Ontario      House\n",
      "...          ...                      ...      ...        ...\n",
      "40763     Ottawa                      NaN  Ontario  Apartment\n",
      "40764   Montreal                      NaN  Ontario  Apartment\n",
      "40765     Ottawa                      NaN  Alberta      House\n",
      "40766     Ottawa                      NaN  Ontario      Condo\n",
      "40767  Vancouver                      NaN  Alberta      Condo\n",
      "\n",
      "[40768 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object'])\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a17f0f5-779f-45e0-b1e7-e4cd4f7c68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocessing for numerical and categorical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())  # Standardize numerical features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d90d69-023b-4a2f-b736-f9aaf9bf12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Combine preprocessors into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd67a5af-816e-4658-b094-7e4047909d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0edad47c-cf63-47fd-ae61-04fd11b5b76d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Toronto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 6: Visualize feature relationships with target\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Correlation matrix\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m combined_data\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(correlation_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:10704\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[1;32m  10702\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m  10703\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m> 10704\u001b[0m mat \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m  10706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m  10707\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:1889\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1888\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m-> 1889\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[1;32m   1891\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1656\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1656\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1715\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1714\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[0;32m-> 1715\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m   1716\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Toronto'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Visualize feature relationships with target\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = combined_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836254a-3051-4483-bc40-91a5ab282e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
